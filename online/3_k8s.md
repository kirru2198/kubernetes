Sure! Here's your session transcription written in clear **Markdown format**, with added **headings**, **code blocks**, and **descriptions** to make it easier to read and review.

---

# üìò Kubernetes Namespace - Training Notes

> *"Namespaces in Kubernetes are a way to divide cluster resources between multiple users and teams."*

---

## üß† Introduction

Hello everyone!  
Good morning / Good evening, depending on your timezone.  
Let's begin the session. Please confirm if you can hear me and see my screen.

---

## üéØ Objective of Today‚Äôs Session

Today, we‚Äôll discuss an important concept in Kubernetes: **Namespaces**.  
We‚Äôll:

- Understand what namespaces are.
- Look at use cases for namespaces.
- Create a namespace and deploy resources into it.
- Compare how resources behave with and without namespaces.

---

## üì¶ What is a Namespace in Kubernetes?

> A **namespace** in Kubernetes is a **virtual cluster** inside a physical Kubernetes cluster.

Think of it like this:
- The **Kubernetes cluster** is your apartment building.
- Each **namespace** is a flat within that apartment.

This allows **logical isolation** of applications and resources for different teams or environments.

---

### üß± Why Use Namespaces?

- To **divide the cluster** into isolated environments.
- To allocate and **restrict resources** (CPU, memory).
- To allow **teams to work independently**.
- Helps with **multi-tenant** scenarios.

---

## üîç Built-In Kubernetes Namespaces

Run this command to view existing namespaces:

```bash
kubectl get namespace
```

You‚Äôll see something like:

```
default
kube-system
kube-public
kube-node-lease
```

### Description of Built-in Namespaces:

| Namespace         | Description |
|-------------------|-------------|
| `default`         | Used when no namespace is specified. |
| `kube-system`     | Used by Kubernetes for system components like kube-proxy, DNS, etc. |
| `kube-public`     | Publicly readable data, accessible by all users. |
| `kube-node-lease` | Used for node heartbeat and lease coordination. |

‚ö†Ô∏è Do **not** manually edit/delete resources in `kube-system`, `kube-node-lease`, etc.

---

## üõ†Ô∏è Creating Your Own Namespace

We can create a namespace using a YAML file:

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: my-namespace
```

### Apply the file:

```bash
kubectl apply -f namespace.yaml
```

### Verify:

```bash
kubectl get namespace
```

You should see your namespace like:

```
my-namespace     Active   7s
```

---

## üöÄ Deploying into a Namespace

Let's create a deployment inside this new namespace.

### nginx-deployment.yaml

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
  namespace: my-namespace
spec:
  replicas: 2
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

### Apply the Deployment:

```bash
kubectl apply -f nginx-deployment.yaml
```

---

## üîç Viewing Resources in a Namespace

By default, `kubectl get pods` will show resources in the **default namespace**.

```bash
kubectl get pods
```

To view pods in a **specific namespace**:

```bash
kubectl get pods -n my-namespace
```

You will see your 2 nginx pods running in `my-namespace`.

---

## ‚úÖ Use Cases for Namespaces

### 1. **Environment Separation**
- `dev`, `staging`, `production` namespaces
- Run multiple versions of the same app in parallel.

### 2. **Team Isolation**
- Team A ‚Üí `team-a` namespace  
- Team B ‚Üí `team-b` namespace  
- Each team can deploy and manage its own resources independently.

### 3. **Resource Quotas & Limits**
- You can assign **CPU/Memory quotas** per namespace.
- Prevent noisy neighbor issues.

### 4. **Cleaner Organization**
- Easier to manage large clusters with many apps.

---

## üß™ Changing the Default Namespace (Context)

By default, commands run in the `default` namespace unless specified.

To **change the context namespace**:

```bash
kubectl config set-context --current --namespace=my-namespace
```

After this, all `kubectl` commands will default to `my-namespace`.

---

## üßº Cleanup

To delete your custom namespace and all resources inside:

```bash
kubectl delete namespace my-namespace
```

---

## üìö References & Lab Link

All step-by-step labs are available in your instructor's **GitHub repository**.  
It includes:

- YAML files
- Use cases
- Commands with examples
- Deep-dive exercises

---

## üîÅ Summary

| Concept | Description |
|--------|-------------|
| Namespace | Logical boundary for resource isolation |
| Default | Used when no namespace is specified |
| Custom Namespace | Can be created via YAML |
| Use Cases | Multi-environment, multi-team, resource quota |
| Tools | `kubectl`, `--namespace`, `set-context` |

---

Here‚Äôs a cleaned-up, organized, and well-structured **Markdown version** of the ConfigMap session content, with descriptions, YAML examples, and clear sections for learning:

---

# üìò Kubernetes ConfigMap Explained

## üîπ What is a ConfigMap?

A **ConfigMap** is a Kubernetes object used to store **non-confidential configuration data** in key-value pairs. It allows you to **decouple configuration** from your container images, making it easier to manage environment-specific or dynamic configurations without rebuilding your Docker images.

---

## üîπ Why Use ConfigMaps?

In real-world applications:

- Configuration often differs between environments (e.g., `dev`, `test`, `prod`).
- Storing configuration inside the Dockerfile leads to image rebuilding for each change.
- ConfigMaps let you change configuration **without modifying your container image**.

### üìå Use Cases:

- Environment variables
- Command-line arguments
- Configuration files

---

## üîπ Key Benefits

| Benefit             | Description                                                                 |
|---------------------|-----------------------------------------------------------------------------|
| Decoupling configs  | Keeps your application code/image separate from environment configs         |
| Reusability         | A single ConfigMap can be reused across multiple pods or deployments        |
| Flexibility         | Modify config values without changing your Docker image                     |
| Environment-specific| Use different ConfigMaps for different environments (e.g., `dev`, `prod`)   |

---

## üîπ Example: Creating a ConfigMap

### üìù `configmap.yaml`

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_ENV: production
  APP_DEBUG: "false"
  LOG_LEVEL: info
```

### ‚úÖ Steps to Create ConfigMap:

```bash
kubectl apply -f configmap.yaml
```

### üîç Verify:

```bash
kubectl get configmaps
kubectl describe configmap app-config
```

---

## üîπ Using ConfigMap in a Pod

You can reference the ConfigMap inside a Pod by injecting values as **environment variables**.

### üìù `pod-env.yaml`

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: configmap-pod
spec:
  containers:
    - name: my-container
      image: busybox
      command: [ "sleep", "3600" ]
      env:
        - name: APP_ENV
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: APP_ENV
        - name: APP_DEBUG
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: APP_DEBUG
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: app-config
              key: LOG_LEVEL
```

### ‚úÖ Apply:

```bash
kubectl apply -f pod-env.yaml
```

### üîç Check the Pod:

```bash
kubectl get pods
kubectl describe pod configmap-pod
```

---

## üîπ Accessing Config Values Inside the Pod

```bash
kubectl exec -it configmap-pod -- /bin/sh
```

Inside the pod:

```bash
echo $APP_ENV        # Output: production
echo $APP_DEBUG      # Output: false
echo $LOG_LEVEL      # Output: info
```

---

## üîπ Updating a ConfigMap

To update a ConfigMap:

1. Modify the YAML file
2. Re-apply it:

```bash
kubectl apply -f configmap.yaml
```

3. Restart your pods (if needed) to pick up the new config:

```bash
kubectl delete pod configmap-pod
```

The newly created pod will pick the updated config.

---

## üîπ Pro Tip

> If your app supports dynamic configuration reloading, it can pick up changes to the ConfigMap without restarting the pod. Otherwise, you‚Äôll need to restart the pod to see the changes.

---

## üîπ Summary

- ConfigMap is a **lightweight and flexible** way to manage configurations.
- Keeps your images clean and **portable across environments**.
- Supports dynamic or static injection into pods.

---

## üìÇ Additional Examples (from GitHub)

> Refer to the provided GitHub repository for more advanced examples:
- Mounting config files as volumes
- Using full configuration files in ConfigMap
- Using with Deployments instead of just Pods

---
Sure! Here's the full explanation of **Kubernetes Secrets** in a well-organized **Markdown** format, including examples, YAML files, and descriptions:

---

# üß™ Kubernetes Secrets

In Kubernetes, **Secrets** are used to store and manage sensitive information, such as passwords, tokens, SSH keys, and certificates. Unlike **ConfigMaps**, Secrets are **base64-encoded** and hidden from direct view to add a layer of security.

---

## üîç Difference Between ConfigMap and Secret

| Feature            | ConfigMap                    | Secret                           |
|--------------------|------------------------------|----------------------------------|
| Purpose            | Store non-sensitive data     | Store sensitive data             |
| Encoding           | Plain text                   | Base64 encoded                   |
| Visible in `kubectl describe` | Yes (key + value shown) | Only key shown (value hidden)    |
| Security Level     | Low                          | Higher than ConfigMap            |

---

## üßæ Use Case

When passing **database credentials** (e.g., username and password), it's not safe to store them in a ConfigMap because they‚Äôre visible in plain text. Instead, use Secrets.

---

## üîê Base64 Encoding

To store values in a Secret, encode them using `base64`:

```bash
# Encode username
echo -n 'myusername' | base64
# Output: bXl1c2VybmFtZQ==

# Encode password
echo -n 'mypassword' | base64
# Output: bXlwYXNzd29yZA==
```

---

## üìÑ secret.yaml

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  username: bXl1c2VybmFtZQ==  # base64 for 'myusername'
  password: bXlwYXNzd29yZA==  # base64 for 'mypassword'
```

- `type: Opaque`: most common type, used for arbitrary key-value pairs.

---

## üõ† Apply the Secret

```bash
kubectl apply -f secret.yaml
```

---

## üìã View the Secret

```bash
kubectl get secrets
kubectl describe secret db-secret
```

### üîí Output:

You will see the **key names** but **not the values**:

```text
Name:         db-secret
Type:         Opaque
Data
====
username: 10 bytes
password: 10 bytes
```

---

## üì¶ Use Secret in a Pod (as Environment Variables)

### pod-env-secret.yaml

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: app
    image: nginx
    env:
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
  restartPolicy: Never
```

### üõ† Apply the Pod

```bash
kubectl apply -f pod-env-secret.yaml
```

---

## üîç Access Secret from Inside Pod

```bash
kubectl exec -it secret-env-pod -- /bin/sh
```

Inside the pod:

```bash
echo $DB_USERNAME  # Outputs: myusername
echo $DB_PASSWORD  # Outputs: mypassword
```

---

## üìö Types of Kubernetes Secrets

| Type              | Description                                |
|-------------------|--------------------------------------------|
| `Opaque`          | Default key-value Secret                   |
| `kubernetes.io/tls` | For storing TLS certs (`tls.crt`, `tls.key`) |
| `kubernetes.io/dockerconfigjson` | For Docker registry credentials   |

---

## ‚úÖ Best Practices

- Always **base64 encode** sensitive values.
- Use `Secrets` for sensitive data instead of `ConfigMaps`.
- Set RBAC rules to restrict who can view secrets.
- Mount secrets as environment variables or as volumes.
- Avoid committing secrets into source control (even in YAML).

---

## üìé Extra Tip: Multi-YAML in One File

You can combine multiple resources in one YAML file using `---`:

```yaml
# secret.yaml
---
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:
  username: bXl1c2VybmFtZQ==
  password: bXlwYXNzd29yZA==
---
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
  - name: app
    image: nginx
    env:
    - name: DB_USERNAME
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: username
    - name: DB_PASSWORD
      valueFrom:
        secretKeyRef:
          name: db-secret
          key: password
  restartPolicy: Never
```

---

## üìñ Documentation Reference

Official Kubernetes documentation on Secrets:
https://kubernetes.io/docs/concepts/configuration/secret/

---
Here‚Äôs your detailed explanation on **DaemonSet** in **Markdown format**, with descriptions and structure maintained for clarity:

---

# üìå Kubernetes DaemonSet Explained

## üîç What is a DaemonSet?

A **DaemonSet** is a Kubernetes resource that ensures a **copy of a specific pod runs on all (or some) nodes** in the cluster. It‚Äôs useful when you need to deploy background system-level services like log collectors, monitoring agents, or networking tools **on every node**.

> üß† **Example Use Case:** Deploying a **log collector** like Fluentd as part of the EFK stack. You want logs collected from every node‚Äîso Fluentd must run on each node.

---

## ü§î Why Not Use Deployment?

Let‚Äôs say you have a Kubernetes cluster with 4 worker nodes and you want to run a pod on each node to collect logs:

- You **can** use a Deployment and set `replicas: 4`, but:
  - The **Kubernetes Scheduler decides** where to run the pods.
  - It‚Äôs **not guaranteed** that each node will get exactly **one pod**.
  - You could end up with 2 pods on one node and none on another.

**Deployments = No guarantee of pod per node distribution.**

> üö´ Not ideal for node-specific services.

---

## ‚úÖ When to Use DaemonSet?

Use **DaemonSet** when you need:

- **One pod per node** (or some nodes).
- Dynamic scaling: if nodes are **added/removed**, DaemonSet ensures pods are created/deleted accordingly.
- **Node-level tasks**: logging, monitoring, node health checks, etc.

---

## üìò Definition of DaemonSet

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

> üßæ **Notes:**
> - `kind: DaemonSet` is the key difference.
> - **No `replicas` field** ‚Äî DaemonSet automatically manages pods per node.
> - Pod template and structure is **same as Deployment**.

---

## üîß Creating DaemonSet: Step-by-Step

### 1. Create the YAML file

File: `nginx-daemonset.yaml`

```yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: nginx-daemonset
spec:
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
```

### 2. Apply the YAML

```bash
kubectl apply -f nginx-daemonset.yaml
```

### 3. Verify DaemonSet

```bash
kubectl get daemonsets
```

You should see something like:

```
NAME              DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
nginx-daemonset   2         2         2       2            2           <none>          7s
```

> ‚úÖ This shows 2 pods running on 2 nodes.

### 4. Get Pods

```bash
kubectl get pods -o wide
```

Look for pods labeled with `nginx-daemonset` ‚Äî one on each node.

---

## üí° Key Differences: Deployment vs DaemonSet

| Feature             | Deployment                     | DaemonSet                      |
|---------------------|---------------------------------|--------------------------------|
| Pods per Node       | Scheduler decides               | One pod per node (guaranteed) |
| Use Case            | Web apps, APIs, microservices   | Log collectors, monitoring    |
| Replicas field      | Required                        | Not used                      |
| Dynamic scaling     | Manual or auto-scaling          | Automatic with node changes   |

---

## üß† Summary

- DaemonSet is used for **node-specific** background tasks.
- Ensures **one pod per node**, useful for logging, monitoring.
- Automatically handles scaling when nodes are added/removed.
- YAML is similar to a Deployment, **minus the `replicas`** and with `kind: DaemonSet`.

> üõ† You‚Äôll often see **Fluentd, Filebeat, node-exporter, kube-proxy** deployed as DaemonSets.

---
Here's a clean and well-structured version of your explanation on **StatefulSets** in Kubernetes, written in **Markdown** with descriptions and examples for better understanding.

---

# üìò Kubernetes: StatefulSet Explained

In Kubernetes, we often deploy applications using objects like `Deployment`, `DaemonSet`, or `StatefulSet`, depending on the application type. Before diving into labs involving persistent storage and state management, it's essential to understand the **StatefulSet** resource.

---

## üß† Stateless vs Stateful Applications

| Application Type | Description | Example |
|------------------|-------------|---------|
| **Stateless**    | Does not store data about previous sessions. Every request is independent. | Web apps, REST APIs |
| **Stateful**     | Maintains information about past interactions (state). Needs consistent identity and storage. | Databases like MySQL, PostgreSQL |

---

## üöÄ Observations from Deployments

- In a **Deployment**:
  - When a **Pod** is deleted, a new one is created with a **different name**.
  - All Pods are created **simultaneously**, without any **ordering**.
  - Good for **stateless** applications.

Example:
```bash
kubectl scale deployment nginx-deploy --replicas=10
```

All pods get created at the same time (check timestamps via `kubectl get pods`).

---

## üóÉÔ∏è Why Not Use Deployment for Databases?

Databases require:
- **Stable identities** (pod names shouldn't change).
- **Ordered pod creation** (Master first, then replicas).
- **Persistent storage**.

Deployments don't guarantee:
- Pod identity persistence.
- Creation order.

Hence, **StatefulSet** is used instead.

---

## üèóÔ∏è Understanding StatefulSet

A **StatefulSet**:
- Maintains **sticky identity** for each pod.
- Provides **stable network identity** and **storage**.
- Ensures **ordered deployment and scaling**.
- Ideal for **stateful** and **distributed** applications.

### üß∞ Use Cases:
- Databases (MySQL, PostgreSQL, MongoDB)
- Distributed systems (Kafka, Zookeeper)

---

## üîß Example: MySQL with StatefulSet

### Step 1: Create a Headless Service (for stable DNS)

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None  # Headless service
  selector:
    app: mysql
  ports:
    - port: 3306
```

### Step 2: Create a StorageClass (optional, based on environment)

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: standard
provisioner: kubernetes.io/aws-ebs  # Or your cloud provider
```

### Step 3: StatefulSet YAML

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: "mysql"
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        ports:
        - containerPort: 3306
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "rootpassword"
        volumeMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: mysql-persistent-storage
    spec:
      accessModes: ["ReadWriteOnce"]
      storageClassName: "standard"
      resources:
        requests:
          storage: 1Gi
```

---

## üîÑ How Pod Creation Works in StatefulSet

1. Pod `mysql-0` (master) is created.
2. Once `mysql-0` is **running**, `mysql-1` (replica 1) is created.
3. After `mysql-1` is **running**, `mysql-2` (replica 2) is created.

> Pod names remain stable: `mysql-0`, `mysql-1`, `mysql-2` (even after deletion and recreation).

---

## ‚úÖ Summary

| Feature                     | Deployment     | StatefulSet    |
|-----------------------------|----------------|----------------|
| Pod Identity                | Ephemeral      | Persistent     |
| Creation Order              | Parallel       | Ordered        |
| Storage                     | Shared/None    | Persistent     |
| Use Case                    | Stateless Apps | Stateful Apps  |

Use **StatefulSet** when:
- Applications require **stable identities**
- Applications depend on **persistent data**
- You need **ordered** deployment or scaling

---

Sure! Here's a **well-structured Markdown document** based on your detailed Kubernetes live session. I've broken it down by topic and added descriptions and formatting for clarity:

---

# üê≥ Kubernetes StatefulSet with MySQL and EFK Stack (Fluentd, Elasticsearch, Kibana)

## üìå Overview

This document outlines a hands-on walkthrough of deploying:
- A **MySQL StatefulSet** in Kubernetes with persistent storage using a `StorageClass`
- An **EFK (Elasticsearch, Fluentd, Kibana)** logging stack with dynamic provisioning and `LoadBalancer` service

---

## üß± Part 1: MySQL StatefulSet Deployment

### ‚úÖ StatefulSet YAML

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql:5.7
        env:
        - name: MYSQL_ROOT_PASSWORD
          value: "password"
        ports:
        - containerPort: 3306
        volumeMounts:
        - name: mysql-data
          mountPath: /var/lib/mysql
  volumeClaimTemplates:
  - metadata:
      name: mysql-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 5Gi
      storageClassName: mysql-storage
```

### üí° Key Concepts

- `replicas: 3` means 3 MySQL pods: `mysql-0`, `mysql-1`, `mysql-2`
- Ordered and **sequential pod creation**:
  - `mysql-0` (master) is created and waits until `Running` state
  - Then `mysql-1` (replica) is created
  - Finally `mysql-2` is created
- `volumeClaimTemplates` ensures each pod gets its own persistent volume (5Gi each)

---

## üåê MySQL Headless Service

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql
spec:
  clusterIP: None
  selector:
    app: mysql
  ports:
  - name: mysql
    port: 3306
```

- **`clusterIP: None`** makes it a headless service ‚Äî required for StatefulSets for stable network identity

---

## ‚öôÔ∏è StorageClass YAML

```yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: mysql-storage
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp2
```

- Used for **dynamic provisioning** of EBS volumes in AWS
- `gp2` is the volume type

---

## üõë Troubleshooting

### PVC Pending?

If the PVC remains in `Pending` state:
- Check if **CSI driver** for EBS is installed
- If not, deploy the AWS EBS CSI driver:
  - GitHub: [aws-ebs-csi-driver](https://github.com/kubernetes-sigs/aws-ebs-csi-driver)
- Ensure your nodes have necessary IAM permissions

---

## üìä Part 2: EFK Stack Deployment

### üìÅ Files Used

Directory contains:

```bash
.
‚îú‚îÄ‚îÄ kibana.yaml
‚îú‚îÄ‚îÄ fluentd.yaml
‚îú‚îÄ‚îÄ elasticsearch-statefulset.yaml
‚îú‚îÄ‚îÄ elasticsearch-configmap.yaml
‚îú‚îÄ‚îÄ kibana-service.yaml
‚îî‚îÄ‚îÄ namespace.yaml
```

### üßæ Namespace Creation

```bash
kubectl create namespace efk-log
```

All components are deployed in this namespace: `efk-log`

---

### üîÑ Apply All Components

```bash
kubectl apply -f .
```

Deploys:
- **Elasticsearch** (StatefulSet with PVCs)
- **Fluentd** (DaemonSet on all nodes)
- **Kibana** (Deployment)
- **LoadBalancer Service** for Kibana

---

## üìå StatefulSet Pod Behavior (Again)

- Elasticsearch runs as a StatefulSet:
  - `logging-0` starts first (primary/master node)
  - Once it's `Running`, `logging-1` initializes
  - Ordered startup is guaranteed

---

## üì∫ Access Kibana

1. Identify external IP:
```bash
kubectl get svc -n efk-log
```

Look for the `EXTERNAL-IP` under the `kibana-logging` LoadBalancer.

2. Open browser:

```
http://<EXTERNAL-IP>:5601
```

Port `5601` is default for Kibana UI

---

## üß† Summary

| Component       | Type         | Description                          |
|----------------|--------------|--------------------------------------|
| MySQL          | StatefulSet  | With PVC and headless service        |
| StorageClass   | Custom       | Enables dynamic provisioning on AWS  |
| Fluentd        | DaemonSet    | Collects logs from all nodes         |
| Elasticsearch  | StatefulSet  | Stores logs, ordered boot            |
| Kibana         | Deployment   | Visualizes logs                      |
| Kibana Service | LoadBalancer | Exposes Kibana dashboard             |

---

## üì¶ Helpful Commands

```bash
# List stateful sets
kubectl get statefulset -n efk-log

# Describe a PVC
kubectl describe pvc <name> -n efk-log

# View all pods in namespace
kubectl get pods -n efk-log

# View services in namespace
kubectl get svc -n efk-log
```
Here's a clean and well-structured Markdown version of the session summary, with clear explanations and descriptions for each concept covered:

---

# üê≥ Kubernetes EFK Stack Deployment Summary

This document outlines the key points discussed in the recent Kubernetes session, focusing on the **EFK stack** ‚Äî Elasticsearch, Fluentd, and Kibana ‚Äî and how it's deployed within Kubernetes using various objects like `Deployment`, `DaemonSet`, `StatefulSet`, and services.

---

## üìå What is Kibana?

- **Kibana** is a visualization tool used to explore data stored in **Elasticsearch**.
- It provides a web interface to perform queries and visualize logs/metrics.
- In Kubernetes, we deploy Kibana as a **Deployment** using a Kibana Docker image.

### ‚úÖ Kibana Setup (Deployment YAML)

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:7.9.2
        ports:
        - containerPort: 5601
        resources:
          limits:
            cpu: "100m"
```

---

## üß± What is Fluentd?

- **Fluentd** is a log collector.
- It collects logs from **all pods and nodes** in the Kubernetes cluster.
- We use a **DaemonSet** for Fluentd to ensure **one pod per node**.

### ‚úÖ Why DaemonSet for Fluentd?

> DaemonSet ensures that every node runs exactly one pod of Fluentd. Since it collects logs from each node, it must be present on all nodes.

---

## üóÉÔ∏è What is Elasticsearch?

- **Elasticsearch** is a NoSQL database and search engine where Fluentd sends logs.
- We deploy it as a **StatefulSet**, because:
  - It maintains **state** (data)
  - Requires persistent storage
  - Needs **stable network identity** and **persistent volumes**

### ‚úÖ Elasticsearch StatefulSet Overview

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: "elasticsearch"
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.9.2
        ports:
        - containerPort: 9200
```

---

## üîó Services and Networking

### Internal Communication

- Done via **ClusterIP** services.
- Kibana ‚Üî Elasticsearch communicate using internal DNS and cluster IPs.

### External Access

- Exposed using a **LoadBalancer** service.
- Allows external traffic (e.g., from a browser) to reach the Kibana dashboard.

### ‚úÖ LoadBalancer Service for Kibana

```yaml
apiVersion: v1
kind: Service
metadata:
  name: kibana-lb
  namespace: logging
spec:
  type: LoadBalancer
  ports:
    - port: 5601
      targetPort: 5601
  selector:
    app: kibana
```

---

## üß™ Use Case Flow: EFK in Action

1. **Fluentd** (as DaemonSet) collects logs from nodes/pods.
2. Fluentd sends logs to **Elasticsearch** (StatefulSet).
3. **Kibana** (Deployment) connects to Elasticsearch and visualizes logs.
4. Kibana UI is accessed via **LoadBalancer** (port `5601`).

---

## ‚ö†Ô∏è Common Error: `connection refused localhost:8080`

### üîç Root Cause

- Typically occurs when the `kubectl` config is not set correctly or user is incorrect.
- `~/.kube/config` file might be missing or misconfigured.

### ‚úÖ Fix

- Ensure you're using the **same user** you created the cluster with.
- Confirm the presence of `.kube/config` under the correct user directory.
- Switch users with `sudo su - <username>` if needed.

---

## üìÅ Next Steps

- Trainer will upload **5‚Äì7 lab exercises** on GitHub.
- Labs will not include solutions upfront ‚Äî try them yourself first!
- Solutions will be provided in a separate folder.

### üìù Suggested Practice

- Review all created resources: PVs, PVCs, StorageClass, Secrets, ConfigMaps
- Try deploying the EFK stack yourself
- Explore the Kibana dashboard
- Prepare questions for the next session

---

## üöÄ Upcoming Topics

- Wrap-up of EFK stack
- Introduction to **Terraform**
- Followed by **Ansible**

> Terraform and Ansible are simple and easy to learn compared to Kubernetes, so keep pushing through the Kubernetes learning curve!

---

## üîó Helpful Links

- ‚úÖ [Trainer‚Äôs GitHub Repository](#) *(Replace with actual URL)*
- üìå Watch for the `lab/` directory updates this weekend.

---
